{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit study data to GEO\n",
    "This Python Jupyter notebook submits the processed and raw study data to the [GEO database](https://www.ncbi.nlm.nih.gov/geo/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Excel metadata template\n",
    "We have already created an Excel metadata workbook by manually filling the [GEO template](https://www.ncbi.nlm.nih.gov/geo/info/examples/seq_template_v2.1.xls) with information appropriate for our experiment.\n",
    "\n",
    "We read this notebook and get the single active worksheet.\n",
    "This sheet has all the relevant information **except** the MD5 checksums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata template Excel workbook\n",
    "wb = openpyxl.load_workbook('metadata_template.xlsx')\n",
    "\n",
    "# get active worksheet, which should be first and only one\n",
    "assert len(wb.sheetnames) == 1, \"multiple notebook sheets\"\n",
    "ws = wb.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MD5 checksums and files to upload\n",
    "We want to parse the worksheet to identify all files that need to be submitted to GEO and then do the following:\n",
    " 1. Add the MD5 checksum to the worksheet\n",
    " 2. Add the file to the list to upload to GEO\n",
    " \n",
    "These needs to be done for two titled sections of the notebook:\n",
    " - a section titled *PROCESSED DATA FILES*\n",
    " - a section titled *RAW FILES*\n",
    " \n",
    "Each of these sections should first have a heading line with the first three columns being *file name*, *file type*, and *file checksum*--with the last of these columns where we add the checksum.\n",
    "Each section ends with the first line that is a comment (begins with `#`).\n",
    "\n",
    "So we go through the notebook and create a dict that is keyed by the names of all processed and raw data files that we need to upload, and has as its values the cell in the worksheet where the MD5 checksum for that file needs to be placed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying PROCESSED DATA FILES files:\n",
      "merged_canine_cells.tsv\n",
      "merged_canine_genes.tsv\n",
      "merged_canine_matrix.mtx\n",
      "merged_humanplusflu_cells.tsv\n",
      "merged_humanplusflu_genes.tsv\n",
      "merged_humanplusflu_matrix.mtx\n",
      "PacBio_annotated_merged_humanplusflu_cells.tsv\n",
      "\n",
      "Identifying RAW FILES files:\n",
      "2017-06-08_ccs.bam\n",
      "2017-06-08_report.csv\n",
      "2017-12-07_ccs.bam\n",
      "2017-12-07_report.csv\n",
      "2018-06-22_nonPol_ccs.bam\n",
      "2018-06-22_nonPol_report.csv\n",
      "2018-06-22_Pol-1_ccs.bam\n",
      "2018-06-22_Pol-1_report.csv\n",
      "2018-06-22_Pol-2_ccs.bam\n",
      "2018-06-22_Pol-2_report.csv\n",
      "2018-06-22_Pol_open_ccs.bam\n",
      "2018-06-22_Pol_open_report.csv\n",
      "2018-08-08_Pol_circ_ccs.bam\n",
      "2018-08-08_Pol_circ_report.csv\n",
      "IFN_enriched_S1_L002_R1_001.fastq.gz\n",
      "IFN_enriched_S1_L002_R2_001.fastq.gz\n",
      "IFN_enriched_S1_L002_I1_001.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "rows = list(ws.rows)\n",
    "\n",
    "files_to_upload = {}\n",
    "\n",
    "for section in ['PROCESSED DATA FILES', 'RAW FILES']:\n",
    "    \n",
    "    print(f\"\\nIdentifying {section} files:\")\n",
    "    \n",
    "    # identify row with section title\n",
    "    titlecell = [row[0] for row in rows if row[0].value == section]\n",
    "    if len(titlecell) != 1:\n",
    "        raise ValueError(f\"not exactly one title row for {section}\")\n",
    "    titlerow = titlecell[0].row\n",
    "    \n",
    "    # check correctness of header following section title\n",
    "    header = [cell.value for cell in rows[titlerow][ : 3]]\n",
    "    if header != ['file name', 'file type', 'file checksum']:\n",
    "        raise ValueError(f\"header incorrect for {section}\")\n",
    "        \n",
    "    # get all rows with files to upload\n",
    "    i = 1 + titlerow\n",
    "    filename = rows[i][0].value\n",
    "    while ((filename is not None) and (filename[0] != '#') and not filename.isspace()):\n",
    "        print(filename)\n",
    "        checksumcell = rows[i][2]\n",
    "        if checksumcell.value is not None:\n",
    "            raise ValueError(\"checksum cell already contains a value\")\n",
    "        files_to_upload[filename] = checksumcell\n",
    "        i += 1\n",
    "        filename = rows[i][0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look for each of these files.\n",
    "We specify directories where they are stored, and make sure each file is uniquely located in one of these directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../results/pacbio/ccs/', '../results/cellgenecounts/']\n"
     ]
    }
   ],
   "source": [
    "# directories where files may be found\n",
    "search_dirs = [\n",
    "        # location of Illumina FASTQ files for transcriptomics\n",
    "        '../results/demultiplexed_reads/2017-07-21/fastq/IFN_enriched/',\n",
    "        # location of PacBio CCS files\n",
    "        '../results/pacbio/ccs/',\n",
    "        # location of cell-gene matrix files\n",
    "        '../results/cellgenecounts/'\n",
    "        ]\n",
    "\n",
    "search_files = []\n",
    "for search_dir in search_dirs:\n",
    "    search_files += glob.glob(f'{search_dir)\n",
    "    \n",
    "print(search_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "none",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
