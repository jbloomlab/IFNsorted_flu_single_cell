{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit study data to GEO\n",
    "This Python Jupyter notebook submits the processed and raw study data to the [GEO database](https://www.ncbi.nlm.nih.gov/geo/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import hashlib\n",
    "import ftplib\n",
    "\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Excel metadata template\n",
    "We have already created an Excel metadata workbook by manually filling the [GEO template](https://www.ncbi.nlm.nih.gov/geo/info/examples/seq_template_v2.1.xls) with information appropriate for our experiment.\n",
    "\n",
    "We read this notebook and get the single active worksheet.\n",
    "This sheet has all the relevant information **except** the MD5 checksums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata template Excel workbook\n",
    "wb = openpyxl.load_workbook('metadata_template.xlsx')\n",
    "\n",
    "# get active worksheet, which should be first and only one\n",
    "assert len(wb.sheetnames) == 1, \"multiple notebook sheets\"\n",
    "ws = wb.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert CCS BAM files to FASTQ\n",
    "The PacBio CCS files are currently stored as `*ccs.bam` files as generated by the `ccs` program.\n",
    "We want to convert them to gzipped FASTQ to upload, and those are what are specified in our mdetadata template.\n",
    "\n",
    "So here we use `samtools` to do this, keeping the tags that give the accuracy and number of passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/pacbio/ccs/2017-06-08_ccs.fastq.gz already exists\n",
      "../results/pacbio/ccs/2018-08-08_Pol_circ_ccs.fastq.gz already exists\n",
      "../results/pacbio/ccs/2018-06-22_Pol-1_ccs.fastq.gz already exists\n",
      "../results/pacbio/ccs/2018-06-22_nonPol_ccs.fastq.gz already exists\n",
      "../results/pacbio/ccs/2018-06-22_Pol_open_ccs.fastq.gz already exists\n",
      "../results/pacbio/ccs/2018-06-22_Pol-2_ccs.fastq.gz already exists\n",
      "../results/pacbio/ccs/2017-12-07_ccs.fastq.gz already exists\n"
     ]
    }
   ],
   "source": [
    "ccsdir = '../results/pacbio/ccs/'\n",
    "\n",
    "for bam_ccs in glob.glob(f'{ccsdir}/*ccs.bam'):\n",
    "    fastq_ccs = os.path.splitext(bam_ccs)[0] + '.fastq.gz'\n",
    "    if not os.path.isfile(fastq_ccs):\n",
    "        print(f\"Creating {fastq_ccs} from {bam_ccs}\")\n",
    "        !samtools bam2fq -T np,rq {bam_ccs} | gzip > {fastq_ccs} \n",
    "        assert os.path.isfile(fastq_ccs)\n",
    "    else:\n",
    "        print(f\"{fastq_ccs} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MD5 checksums and files to upload\n",
    "We want to parse the worksheet to identify all files that need to be submitted to GEO and then do the following:\n",
    " 1. Add the MD5 checksum to the worksheet\n",
    " 2. Add the file to the list to upload to GEO\n",
    " \n",
    "These needs to be done for two titled sections of the notebook:\n",
    " - a section titled *PROCESSED DATA FILES*\n",
    " - a section titled *RAW FILES*\n",
    " \n",
    "Each of these sections should first have a heading line with the first three columns being *file name*, *file type*, and *file checksum*--with the last of these columns where we add the checksum.\n",
    "Each section ends with the first line that is a comment (begins with `#`).\n",
    "\n",
    "So we go through the notebook and create a dict that is keyed by the names of all processed and raw data files that we need to upload, and has as its values the cell in the worksheet where the MD5 checksum for that file needs to be placed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identifying PROCESSED DATA FILES files:\n",
      "merged_canine_cells.tsv\n",
      "merged_canine_genes.tsv\n",
      "merged_canine_matrix.mtx\n",
      "merged_humanplusflu_cells.tsv\n",
      "merged_humanplusflu_genes.tsv\n",
      "merged_humanplusflu_matrix.mtx\n",
      "PacBio_annotated_merged_humanplusflu_cells.tsv\n",
      "\n",
      "Identifying RAW FILES files:\n",
      "2017-06-08_ccs.fastq.gz\n",
      "2017-06-08_report.csv\n",
      "2017-12-07_ccs.fastq.gz\n",
      "2017-12-07_report.csv\n",
      "2018-06-22_nonPol_ccs.fastq.gz\n",
      "2018-06-22_nonPol_report.csv\n",
      "2018-06-22_Pol-1_ccs.fastq.gz\n",
      "2018-06-22_Pol-1_report.csv\n",
      "2018-06-22_Pol-2_ccs.fastq.gz\n",
      "2018-06-22_Pol-2_report.csv\n",
      "2018-06-22_Pol_open_ccs.fastq.gz\n",
      "2018-06-22_Pol_open_report.csv\n",
      "2018-08-08_Pol_circ_ccs.fastq.gz\n",
      "2018-08-08_Pol_circ_report.csv\n",
      "IFN_enriched_S1_L002_R1_001.fastq.gz\n",
      "IFN_enriched_S1_L002_R2_001.fastq.gz\n",
      "IFN_enriched_S1_L002_I1_001.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "rows = list(ws.rows)\n",
    "\n",
    "files_to_upload = {}\n",
    "\n",
    "for section in ['PROCESSED DATA FILES', 'RAW FILES']:\n",
    "    \n",
    "    print(f\"\\nIdentifying {section} files:\")\n",
    "    \n",
    "    # identify row with section title\n",
    "    titlecell = [row[0] for row in rows if row[0].value == section]\n",
    "    if len(titlecell) != 1:\n",
    "        raise ValueError(f\"not exactly one title row for {section}\")\n",
    "    titlerow = titlecell[0].row\n",
    "    \n",
    "    # check correctness of header following section title\n",
    "    header = [cell.value for cell in rows[titlerow][ : 3]]\n",
    "    if header != ['file name', 'file type', 'file checksum']:\n",
    "        raise ValueError(f\"header incorrect for {section}\")\n",
    "        \n",
    "    # get all rows with files to upload\n",
    "    i = 1 + titlerow\n",
    "    filename = rows[i][0].value\n",
    "    while ((filename is not None) and (filename[0] != '#') and not filename.isspace()):\n",
    "        print(filename)\n",
    "        checksumcell = rows[i][2]\n",
    "        if checksumcell.value is not None:\n",
    "            raise ValueError(\"checksum cell already contains a value\")\n",
    "        files_to_upload[filename] = checksumcell\n",
    "        i += 1\n",
    "        filename = rows[i][0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look for each of these files.\n",
    "We specify directories where they are stored, and make sure each file is uniquely located in one of these directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories where files may be found\n",
    "search_dirs = [\n",
    "        # location of Illumina FASTQ files for transcriptomics\n",
    "        '../results/demultiplexed_reads/2017-07-21/fastq/IFN_enriched/',\n",
    "        # location of PacBio CCS files\n",
    "        ccsdir,\n",
    "        # location of cell-gene matrix files\n",
    "        '../results/cellgenecounts/'\n",
    "        ]\n",
    "\n",
    "search_files = []\n",
    "for search_dir in search_dirs:\n",
    "    search_files += glob.glob(f'{search_dir}/*')\n",
    "    \n",
    "# get full path to each file\n",
    "fullpaths = {}\n",
    "for fbase, cell in files_to_upload.items():\n",
    "    for fullf in search_files:\n",
    "        if os.path.basename(fullf) == os.path.basename(fbase):\n",
    "            if fbase not in fullpaths:\n",
    "                fullpaths[fbase] = fullf\n",
    "            else:\n",
    "                raise ValueError(f\"Found multiple occurrences of {fbase}\")\n",
    "    if fbase not in fullpaths:\n",
    "        raise ValueError(f\"Failed to find full path for {fbase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the MD5 checksum for each file and fill back into the Excel worksheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD5 checksum for merged_canine_cells.tsv is 44f0cc89e99d228ad8779caee0c7fd20\n",
      "MD5 checksum for merged_canine_genes.tsv is 4aebdd28ed1a69be3153e8b683cedaf6\n",
      "MD5 checksum for merged_canine_matrix.mtx is a77c1dad0ab19d0ab1fba006a3bc8b2d\n",
      "MD5 checksum for merged_humanplusflu_cells.tsv is 726c40311766d48f0bf15deea86c4d30\n",
      "MD5 checksum for merged_humanplusflu_genes.tsv is 3676d0cbd61565f4712b442d692a4ca7\n",
      "MD5 checksum for merged_humanplusflu_matrix.mtx is c7abf72c769f7e8c26f104683a162de3\n",
      "MD5 checksum for PacBio_annotated_merged_humanplusflu_cells.tsv is ed70abd15bca32bc373d1f656704b5cb\n",
      "MD5 checksum for 2017-06-08_ccs.fastq.gz is e19c98ff793be4685595b584531ed36b\n",
      "MD5 checksum for 2017-06-08_report.csv is 26d1727811f15b380fc0bd26630e8898\n",
      "MD5 checksum for 2017-12-07_ccs.fastq.gz is 8fde9acce7dc5f4db84b6b8a9fbb36c3\n",
      "MD5 checksum for 2017-12-07_report.csv is f3ef600db9558a3325ac7e1c02514778\n",
      "MD5 checksum for 2018-06-22_nonPol_ccs.fastq.gz is 5674846b3243c125af2245b2a84bd7d0\n",
      "MD5 checksum for 2018-06-22_nonPol_report.csv is 91a15217144bc02200852e7d14a08500\n",
      "MD5 checksum for 2018-06-22_Pol-1_ccs.fastq.gz is 2660b4ab498f6c8dfebadad5cdc0800b\n",
      "MD5 checksum for 2018-06-22_Pol-1_report.csv is a5f9198420a5a536e2d5c2b8234ecdef\n",
      "MD5 checksum for 2018-06-22_Pol-2_ccs.fastq.gz is 25ac56328cfb8945bc38258886db42a8\n",
      "MD5 checksum for 2018-06-22_Pol-2_report.csv is bfb7b0da4a910784d894f2b30e80e7a8\n",
      "MD5 checksum for 2018-06-22_Pol_open_ccs.fastq.gz is 3fc3561ece838cbfbdd42973bf6769d8\n",
      "MD5 checksum for 2018-06-22_Pol_open_report.csv is 3377e205e5534c585be7d1844259a369\n",
      "MD5 checksum for 2018-08-08_Pol_circ_ccs.fastq.gz is b2c563048a230070895e4dde0565b9aa\n",
      "MD5 checksum for 2018-08-08_Pol_circ_report.csv is 54c62f99e5960f5e498e95e7a17f2d6d\n",
      "MD5 checksum for IFN_enriched_S1_L002_R1_001.fastq.gz is 42e57b7bfdf0e8dfb5d8ef04b712759b\n",
      "MD5 checksum for IFN_enriched_S1_L002_R2_001.fastq.gz is c5cee9585096dbe7cfc54d01da0c1144\n",
      "MD5 checksum for IFN_enriched_S1_L002_I1_001.fastq.gz is 891edbee3c77556efa3b30457b3e2424\n"
     ]
    }
   ],
   "source": [
    "# copied from here: https://stackoverflow.com/a/11143944\n",
    "def md5sum(filename):\n",
    "    md5 = hashlib.md5()\n",
    "    with open(filename, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(128 * md5.block_size), b''):\n",
    "            md5.update(chunk)\n",
    "    return md5.hexdigest()\n",
    "\n",
    "for fbase, fullf in fullpaths.items():\n",
    "    md5checksum = md5sum(fullf)\n",
    "    print(f\"MD5 checksum for {fbase} is {md5checksum}\")\n",
    "    files_to_upload[fbase].value = md5checksum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save the Excel workbook with the filled values to a new name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.xlsx is now an Excel workbook with the completed metadata.\n"
     ]
    }
   ],
   "source": [
    "completed_metadata = 'metadata.xlsx'\n",
    "wb.save(filename=completed_metadata)\n",
    "\n",
    "print(f\"{completed_metadata} is now an Excel workbook with the completed metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now upload files to GEO\n",
    "We now upload the files to GEO via FTP following the [instructions here](https://www.ncbi.nlm.nih.gov/geo/info/submissionftp.html).\n",
    "\n",
    "Note that this requires a username and password.\n",
    "For security reasons, those are not in this notebook but are in a file called `GEO_password.txt` in this directory (but **not** tracked on GitHub) with the first line being the username and the second being the password.\n",
    "In order to run this notebook, you need to create this file yourself with a valid username and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GEO_password.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    if len(lines) != 2:\n",
    "        raise ValueError(\"Did not find exactly two lines\")\n",
    "    username = lines[0].strip()\n",
    "    password = lines[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we connect to the FTP server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp = ftplib.FTP(host='ftp-private.ncbi.nlm.nih.gov',\n",
    "                 user=username,\n",
    "                 passwd=password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a directory with our GEO username as instructed on the website.\n",
    "For me, this is jbloom@fhcrc.org.\n",
    "We then transfer all the files to this directory, and print the contents of the directory after transfer.\n",
    "Finally, we close the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are files transferred to directory /jbloom@fhcrc.org:\n",
      "-rw-rw-r--   1 geo      geo       8876563 Oct 26 18:47 2017-06-08_ccs.fastq.gz\n",
      "-rw-rw-r--   1 geo      geo           832 Oct 26 18:47 2017-06-08_report.csv\n",
      "-rw-rw-r--   1 geo      geo      64380085 Oct 26 18:47 2017-12-07_ccs.fastq.gz\n",
      "-rw-rw-r--   1 geo      geo           822 Oct 26 18:47 2017-12-07_report.csv\n",
      "-rw-rw-r--   1 geo      geo      51120866 Oct 26 18:47 2018-06-22_Pol-1_ccs.fastq.gz\n",
      "-rw-rw-r--   1 geo      geo           818 Oct 26 18:47 2018-06-22_Pol-1_report.csv\n",
      "-rw-rw-r--   1 geo      geo      60266203 Oct 26 18:48 2018-06-22_Pol-2_ccs.fastq.gz\n",
      "-rw-rw-r--   1 geo      geo           819 Oct 26 18:48 2018-06-22_Pol-2_report.csv\n",
      "-rw-rw-r--   1 geo      geo      69642418 Oct 26 18:48 2018-06-22_Pol_open_ccs.fastq.gz\n",
      "-rw-rw-r--   1 geo      geo           820 Oct 26 18:48 2018-06-22_Pol_open_report.csv\n",
      "-rw-rw-r--   1 geo      geo      66592996 Oct 26 18:47 2018-06-22_nonPol_ccs.fastq.gz\n",
      "-rw-rw-r--   1 geo      geo           820 Oct 26 18:47 2018-06-22_nonPol_report.csv\n",
      "-rw-rw-r--   1 geo      geo      110383291 Oct 26 18:48 2018-08-08_Pol_circ_ccs.fastq.gz\n",
      "-rw-rw-r--   1 geo      geo           821 Oct 26 18:48 2018-08-08_Pol_circ_report.csv\n",
      "-rw-rw-r--   1 geo      geo      72753880 Oct 26 18:49 IFN_enriched_S1_L002_I1_001.fastq.gz\n",
      "-rw-rw-r--   1 geo      geo      225734303 Oct 26 18:48 IFN_enriched_S1_L002_R1_001.fastq.gz\n",
      "-rw-rw-r--   1 geo      geo      624332621 Oct 26 18:49 IFN_enriched_S1_L002_R2_001.fastq.gz\n",
      "-rw-rw-r--   1 geo      geo        764377 Oct 26 18:47 PacBio_annotated_merged_humanplusflu_cells.tsv\n",
      "-rw-rw-r--   1 geo      geo         10157 Oct 26 18:47 merged_canine_cells.tsv\n",
      "-rw-rw-r--   1 geo      geo        566498 Oct 26 18:47 merged_canine_genes.tsv\n",
      "-rw-rw-r--   1 geo      geo       1361663 Oct 26 18:47 merged_canine_matrix.mtx\n",
      "-rw-rw-r--   1 geo      geo        249005 Oct 26 18:47 merged_humanplusflu_cells.tsv\n",
      "-rw-rw-r--   1 geo      geo        453410 Oct 26 18:47 merged_humanplusflu_genes.tsv\n",
      "-rw-rw-r--   1 geo      geo      73104287 Oct 26 18:47 merged_humanplusflu_matrix.mtx\n",
      "-rw-rw-r--   1 geo      geo         16157 Oct 26 18:47 metadata.xlsx\n",
      "226 Transfer complete\n"
     ]
    }
   ],
   "source": [
    "dirname = 'jbloom@fhcrc.org'\n",
    "if dirname not in ftp.nlst():\n",
    "    ftp.mkd(dirname)\n",
    "ftp.cwd(dirname)\n",
    "\n",
    "with open(completed_metadata, 'rb') as f:\n",
    "    ftp.storbinary('STOR metadata.xlsx', f)\n",
    "for fbase, fullf in fullpaths.items():\n",
    "    with open(fullf,  'rb') as f:\n",
    "        ftp.storbinary(f'STOR {fbase}', f)\n",
    "\n",
    "print(f\"Here are files transferred to directory {ftp.pwd()}:\")\n",
    "print(ftp.retrlines('LIST'))\n",
    "\n",
    "ftp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notify GEO that data have been transferred!\n",
    "Finally, remember to notify GEO that the data have been transferred [as described here](https://submit.ncbi.nlm.nih.gov/geo/submission/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
